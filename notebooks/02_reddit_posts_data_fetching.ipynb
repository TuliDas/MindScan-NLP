{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install praw\n",
        "!pip install asyncpraw\n",
        "!pip install langdetect"
      ],
      "metadata": {
        "id": "iorFYD7sQF7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "import os\n",
        "\n",
        "import praw\n",
        "from time import sleep\n",
        "\n",
        "from langdetect import detect, DetectorFactory"
      ],
      "metadata": {
        "id": "o0qtner1-yej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "TARGET_PER_LABEL = 2200\n",
        "MAX_LENGTH = 800             # max characters per post\n",
        "SLEEP_TIME = 0.2             # seconds between requests to avoid rate limit\n",
        "DetectorFactory.seed = 0  # for consistent results"
      ],
      "metadata": {
        "id": "hqIx1CoLApdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2   \n",
        "fetching 4 class from reddit\n"
      ],
      "metadata": {
        "id": "cTvtlapRBJnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_hf = pd.read_csv('/content/huggingFace_mental_health_dataset.csv')\n",
        "save_file = \"reddit_mental_health_dataset.csv\"\n",
        "\n",
        "if os.path.exists(save_file):\n",
        "    df_existing = pd.read_csv(save_file)\n",
        "    data_reddit = df_existing.to_dict(\"records\") + df_hf.to_dict(\"records\")\n",
        "    seen_texts = set(df_existing[\"body\"].tolist()) | set(df_hf[\"body\"].tolist())  # avoid duplicates\n",
        "    print(f\"üîÑ Resuming... already have {len(df_existing)} posts\")\n",
        "else:\n",
        "    data_reddit = df_hf.to_dict(\"records\")\n",
        "    seen_texts = set(df_hf[\"body\"].tolist())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2SY7vSPCYbzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Setup Reddit API\n",
        "reddit = praw.Reddit(client_id=\"YOUR_CLIENT_ID\",\n",
        "                     client_secret=\"YOUR_CLIENT_SECRET\",\n",
        "                     user_agent=\"YOUR_USER_AGENT\")"
      ],
      "metadata": {
        "id": "AB2wNt5cDIef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Define rest of subreddits\n",
        "\n",
        "mental_subreddits = {\n",
        "    \"Anxiety\": {\n",
        "        \"sub_list\": [\"anxiety\", \"socialanxiety\", \"PanicAttack\", \"HealthAnxiety\"],\n",
        "        \"MIN_LENGTH\": 80\n",
        "    },\n",
        "    \"Suicidal\": {\n",
        "        \"sub_list\": [\"SuicideWatch\", \"Suicidal_Thoughts\", \"SuicideBereavement\"],\n",
        "        \"MIN_LENGTH\": 50\n",
        "    },\n",
        "    \"Addiction\": {\n",
        "        \"sub_list\": [\"StopSmoking\", \"addiction\", \"StopDrinking\", \"stopgaming\"],\n",
        "        \"MIN_LENGTH\": 80\n",
        "    },\n",
        "    \"Normal\": {\n",
        "        \"sub_list\": [\"happy\", \"CasualConversation\", \"wholesomememes\", \"LifeProTips\"],\n",
        "        \"MIN_LENGTH\": 50\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "cyGtwQ0mDKgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_english(text):\n",
        "    try:\n",
        "        return detect(text) == \"en\"\n",
        "    except:\n",
        "        return False"
      ],
      "metadata": {
        "id": "G7fVheNBDNGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_posts_for_label(label, subreddits, MIN_LENGTH):\n",
        "\n",
        "    for subreddit_name in subreddits:\n",
        "        try:\n",
        "            subreddit = reddit.subreddit(subreddit_name)\n",
        "            print(f\"\\nFetching '{label}' posts from r/{subreddit_name}...\")\n",
        "\n",
        "            for post in subreddit.new(limit=None):\n",
        "                title = post.title.strip()\n",
        "                body = post.selftext.strip()\n",
        "\n",
        "                if len(body) < MIN_LENGTH or len(body) > MAX_LENGTH or body in seen_texts:\n",
        "                    continue\n",
        "                if not is_english(body):\n",
        "                    continue\n",
        "\n",
        "                seen_texts.add(body)\n",
        "\n",
        "                data_reddit.append({\n",
        "                    \"post_id\": post.id,\n",
        "                    \"subreddit\": subreddit_name,\n",
        "                    \"title\": title,\n",
        "                    \"body\": body,\n",
        "                    \"label\": label\n",
        "                })\n",
        "\n",
        "                sleep(SLEEP_TIME)\n",
        "\n",
        "            # Save after each subreddit\n",
        "            df_reddit = pd.DataFrame(data_reddit)\n",
        "            df_reddit.to_csv(save_file, index=False)\n",
        "            print(f\"üíæ Progress saved: {len(df_reddit)} posts so far\")\n",
        "\n",
        "            current_label_count = sum(1 for d in data_reddit if d[\"label\"] == label)\n",
        "            if current_label_count >= TARGET_PER_LABEL:\n",
        "              return\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Could not fetch from r/{subreddit_name}: {e}\")\n"
      ],
      "metadata": {
        "id": "JbTjyaJgDPgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example fetch call\n",
        "for label, subreddits in mental_subreddits.items():\n",
        "    fetch_posts_for_label(label, subreddits[\"sub_list\"], subreddits[\"MIN_LENGTH\"])\n"
      ],
      "metadata": {
        "id": "ce7aP-n8DRrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_reddit = pd.DataFrame(data_reddit)\n",
        "df_reddit = df_reddit.drop_duplicates(subset=\"body\").reset_index(drop=True)\n",
        "df_reddit['label'].value_counts()"
      ],
      "metadata": {
        "id": "bUIkm9UtDTbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BALANCE_SIZE = 2000\n",
        "\n",
        "balanced_df = (\n",
        "    df_reddit\n",
        "    .groupby(\"label\", group_keys=False)   # group by label\n",
        "    .apply(lambda x: x.sample(n=min(BALANCE_SIZE, len(x)), random_state=42))  # sample within each class\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "balanced_df[\"label\"].value_counts()"
      ],
      "metadata": {
        "id": "-2RWzMk52-aP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "balanced_df.to_csv(save_file, index=False)\n",
        "files.download(save_file)"
      ],
      "metadata": {
        "id": "G9osxx6TWA0v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}